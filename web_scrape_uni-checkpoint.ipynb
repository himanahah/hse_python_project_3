{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World University Rankings Data\n",
    "\n",
    "This notebook will lead you through a simple code to extract the data of university ranking from https://www.timeshighereducation.com/\n",
    "________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aim\n",
    "This code tries to extract information from a university ranking website. The concept of collecting data from websites is called web scraping and is used mainly to collect data from websites which do not offer an API to collect data natively. \n",
    "Several tutorials are available to explain the web scraping basics. This notebook is more of a case study. <br />\n",
    "Let's start ......  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prerequisites\n",
    "This code assumes that you have python installed on your machine.  Basic knowledge of python is also assumed. Here is a full list of the prerequisites: \n",
    "* python 3.6 or above\n",
    "* Jupyter notebook - or any environment that allows running python\n",
    "* The following python libraries (BeautifulSoup, Selenium, urllib, objectpath and Pandas) \n",
    "* A web browser, I am using chrome 77 here, but you can use other browsers too\n",
    "* Web driver for the browsers you are using, for chrome and chrome based browsers you can download it from here https://chromedriver.chromium.org/downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What data are you trying to get? \n",
    "This is the first question you should ask yourself, before even touching a single key. In our case, we started with the idea of collecting the list of universities with their ranking. To understand how to do so you will need to visit the website itself to understand a bit about it and its webpages. <br/>\n",
    "\n",
    "The page we are tyring to scrap looked something like this \n",
    "\n",
    "![title](img/basic_page_01.PNG)\n",
    "<br/> <br/> <br/> \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the page contains some sort of a table that hosts the information we are trying to collect. However collecting the information will depend on the HTML code hidden behind what we can see in the browser window. In chrome to display the HTML code simply press F12. The page should look something like this  \n",
    "\n",
    "![title](img/page_code_02.PNG)\n",
    "<br/> <br/> <br/> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the small inspection cursor you can point at elements of the page and find out which part of the HTML represent them. This important because we will only use the HTML to collect the data and not the displayed page in the browser. Once you have identified the part of HTML corresponds with the information we need then we will start scraping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Let's write some python "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A standard method of using python to request internet pages is through the requests library, however in our particular case this approach will not work, because the website uses AJAX to modify the HTML of the page. This means that the HTML code which you will receive by using requests will only contain an empty template of the table and not the information we are trying to collect. To give the JS code a chance to run and populate the table with the information, we use selenium. Selenium uses browsers to request webpages and then collect the HTML after the page is fully loaded, which will allow us to collect the information we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "import json\n",
    "import time\n",
    "\n",
    "# import third party libraries\n",
    "import objectpath\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup as soup\n",
    "from urllib.request import urlopen \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selenium requires a webdriver to access the web browser. It can be installed or used from an executable file directly. In this example we will use the executable file directly, please edit the following code by adding the location of the webdriver. It is recommonded to place it with the code itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining which web address we are going to use for scrapping is essential for the workflow. A quick inspection of the target web address reveals that changing the length parameter from 25 to -1 will result in collecting all the available universities instead of 25 per page. This should enable us to collect the information we need in one go rather than requesting several web pages. \n",
    "Also checking the tabs available on the web page (ranking, scores) reveals more data to be collected. Therefore we will use two web adresses to collect the data, as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_stats = 'https://www.timeshighereducation.com/world-university-rankings/2026/world-ranking#!/page/0/length/-1/sort_by/rank/sort_order/asc/cols/stats'\n",
    "url_scores = 'https://www.timeshighereducation.com/world-university-rankings/2026/world-ranking#!/page/0/length/-1/sort_by/rank/sort_order/asc/cols/scores'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two webdriver objects, one for each of the two adresses \n",
    "stats_browser = webdriver.Chrome()\n",
    "scores_browser = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the follwoing two cells, we will follow the same procedures for each of the two pages as follows: \n",
    "* Request the webpage using the webdriver \n",
    "* Collect the HTML code of the webpage \n",
    "* Use BeautifulSoup to \"parse\" the HTML. This will enable us to collect spesific pices of code after\n",
    "* Use \"findAll\" method to collect the objects which we identified in the HTML code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the webdriver to request the ranking webpage\n",
    "stats_browser.get(url_stats)\n",
    "\n",
    "# collect the webpage HTML after its loading\n",
    "stats_page_html = stats_browser.page_source\n",
    "\n",
    "# parse the HTML using BeautifulSoup\n",
    "stats_page_soup = soup(stats_page_html, 'html.parser')\n",
    "\n",
    "# collect HTML objects \n",
    "rank_obj = stats_page_soup.find_all(\"td\", {\"class\":\"rank sorting_1 sorting_2\"})\n",
    "names_obj = stats_page_soup.find_all(\"td\", {\"class\":\"name namesearch\"})\n",
    "stats_number_students_obj = stats_page_soup.find_all(\"td\", {\"class\":\"stats stats_number_students\"})\n",
    "stats_student_staff_ratio_obj = stats_page_soup.find_all(\"td\", {\"class\":\"stats stats_student_staff_ratio\"})\n",
    "stats_pc_intl_students_obj = stats_page_soup.find_all(\"td\", {\"class\":\"stats stats_pc_intl_students\"})\n",
    "stats_female_male_ratio_obj = stats_page_soup.find_all(\"td\", {\"class\":\"stats stats_female_male_ratio\"})\n",
    "\n",
    "# close the browser\n",
    "stats_browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchWindowException",
     "evalue": "Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=143.0.7499.40)\nStacktrace:\n0   chromedriver                        0x00000001026bee10 cxxbridge1$str$ptr + 3028012\n1   chromedriver                        0x00000001026b6cd0 cxxbridge1$str$ptr + 2994924\n2   chromedriver                        0x00000001021b2b1c _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74196\n3   chromedriver                        0x000000010218c8c4 chromedriver + 149700\n4   chromedriver                        0x0000000102221a88 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 528704\n5   chromedriver                        0x000000010223a4dc _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 629652\n6   chromedriver                        0x00000001021ee17c _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 317492\n7   chromedriver                        0x0000000102683360 cxxbridge1$str$ptr + 2783612\n8   chromedriver                        0x0000000102686ac4 cxxbridge1$str$ptr + 2797792\n9   chromedriver                        0x00000001026635f4 cxxbridge1$str$ptr + 2653200\n10  chromedriver                        0x0000000102687334 cxxbridge1$str$ptr + 2799952\n11  chromedriver                        0x0000000102653fc4 cxxbridge1$str$ptr + 2590176\n12  chromedriver                        0x00000001026a624c cxxbridge1$str$ptr + 2926696\n13  chromedriver                        0x00000001026a63cc cxxbridge1$str$ptr + 2927080\n14  chromedriver                        0x00000001026b6928 cxxbridge1$str$ptr + 2993988\n15  libsystem_pthread.dylib             0x000000019bd07c0c _pthread_start + 136\n16  libsystem_pthread.dylib             0x000000019bd02b80 thread_start + 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNoSuchWindowException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# use the webdriver to request the scores webpage\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mscores_browser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_scores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# collect the webpage HTML after its loading\u001b[39;00m\n\u001b[32m      5\u001b[39m scores_page_html = scores_browser.page_source\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/selenium/webdriver/remote/webdriver.py:483\u001b[39m, in \u001b[36mWebDriver.get\u001b[39m\u001b[34m(self, url)\u001b[39m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    466\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Navigate the browser to the specified URL in the current window or\u001b[39;00m\n\u001b[32m    467\u001b[39m \u001b[33;03m    tab.\u001b[39;00m\n\u001b[32m    468\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    481\u001b[39m \u001b[33;03m    >>> driver.get(\"https://example.com\")\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43murl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/selenium/webdriver/remote/webdriver.py:458\u001b[39m, in \u001b[36mWebDriver.execute\u001b[39m\u001b[34m(self, driver_command, params)\u001b[39m\n\u001b[32m    455\u001b[39m response = cast(RemoteConnection, \u001b[38;5;28mself\u001b[39m.command_executor).execute(driver_command, params)\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merror_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m     response[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._unwrap_value(response.get(\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    460\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/selenium/webdriver/remote/errorhandler.py:233\u001b[39m, in \u001b[36mErrorHandler.check_response\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m    231\u001b[39m         alert_text = value[\u001b[33m\"\u001b[39m\u001b[33malert\u001b[39m\u001b[33m\"\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    232\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[31mNoSuchWindowException\u001b[39m: Message: no such window: target window already closed\nfrom unknown error: web view not found\n  (Session info: chrome=143.0.7499.40)\nStacktrace:\n0   chromedriver                        0x00000001026bee10 cxxbridge1$str$ptr + 3028012\n1   chromedriver                        0x00000001026b6cd0 cxxbridge1$str$ptr + 2994924\n2   chromedriver                        0x00000001021b2b1c _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 74196\n3   chromedriver                        0x000000010218c8c4 chromedriver + 149700\n4   chromedriver                        0x0000000102221a88 _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 528704\n5   chromedriver                        0x000000010223a4dc _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 629652\n6   chromedriver                        0x00000001021ee17c _RNvCsgXDX2mvAJAg_7___rustc35___rust_no_alloc_shim_is_unstable_v2 + 317492\n7   chromedriver                        0x0000000102683360 cxxbridge1$str$ptr + 2783612\n8   chromedriver                        0x0000000102686ac4 cxxbridge1$str$ptr + 2797792\n9   chromedriver                        0x00000001026635f4 cxxbridge1$str$ptr + 2653200\n10  chromedriver                        0x0000000102687334 cxxbridge1$str$ptr + 2799952\n11  chromedriver                        0x0000000102653fc4 cxxbridge1$str$ptr + 2590176\n12  chromedriver                        0x00000001026a624c cxxbridge1$str$ptr + 2926696\n13  chromedriver                        0x00000001026a63cc cxxbridge1$str$ptr + 2927080\n14  chromedriver                        0x00000001026b6928 cxxbridge1$str$ptr + 2993988\n15  libsystem_pthread.dylib             0x000000019bd07c0c _pthread_start + 136\n16  libsystem_pthread.dylib             0x000000019bd02b80 thread_start + 8\n"
     ]
    }
   ],
   "source": [
    "# use the webdriver to request the scores webpage\n",
    "scores_browser.get(url_scores)\n",
    "\n",
    "# collect the webpage HTML after its loading\n",
    "scores_page_html = scores_browser.page_source\n",
    "scores_page_soup = soup(scores_page_html, 'html.parser')\n",
    "\n",
    "# parse the HTML using BeautifulSoup\n",
    "overall_score_obj = scores_page_soup.find_all(\"td\", {\"class\":\"scores overall-score\"})\n",
    "teaching_score_obj = scores_page_soup.find_all(\"td\", {\"class\":\"scores teaching-score\"})\n",
    "research_score_obj = scores_page_soup.find_all(\"td\", {\"class\":\"scores research-score\"})\n",
    "citations_score_obj = scores_page_soup.find_all(\"td\", {\"class\":\"scores citations-score\"})\n",
    "industry_income_score_obj = scores_page_soup.find_all(\"td\", {\"class\":\"scores industry_income-score\"})\n",
    "international_outlook_score_obj = scores_page_soup.find_all(\"td\", {\"class\":\"scores international_outlook-score\"})\n",
    "\n",
    "# close the browser\n",
    "scores_browser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the HTML objects are collected, then we can start extracting the data from them. The data will be presented eventually in pandas dataframe, which can be presented as a table. Pandas dataframe can be constructed using lists of equal length. In the follwoing two cells we will extract/collect the data using two differnet methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting data from HTML objects: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank, names, number_students, student_staff_ratio, intl_students, female_male_ratio, web_address =  [], [], [], [], [], [], []\n",
    "overall_score, teaching_score, research_score, citations_score, industry_income_score, international_outlook_score = [], [], [], [], [], []\n",
    "for i in range(len(names_obj)):\n",
    "    web_address.append('https://www.timeshighereducation.com' + names_obj[i].a.get('href'))\n",
    "    rank.append(rank_obj[i].text)\n",
    "    \n",
    "    names.append(names_obj[i].a.text)\n",
    "    number_students.append(stats_number_students_obj[i].text)\n",
    "    student_staff_ratio.append(stats_student_staff_ratio_obj[i].text)\n",
    "    intl_students.append(stats_pc_intl_students_obj[i].text)\n",
    "    female_male_ratio.append(stats_female_male_ratio_obj[i].text[:2])\n",
    "    \n",
    "    overall_score.append(overall_score_obj[i].text)\n",
    "    teaching_score.append(teaching_score_obj[i].text)\n",
    "    research_score.append(research_score_obj[i].text)\n",
    "    citations_score.append(citations_score_obj[i].text)\n",
    "    industry_income_score.append(industry_income_score_obj[i].text)\n",
    "    international_outlook_score.append(international_outlook_score_obj[i].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_address_list, streetAddress_list, addressLocality_list, addressRegion_list, postalCode_list, addressCountry_list  = [], [], [], [], [], []\n",
    "for web in web_address:\n",
    "    page = urlopen(web)\n",
    "    page_html = soup(page, 'html.parser')\n",
    "    location = page_html.findAll('script', {'type':\"application/ld+json\"})\n",
    "    jt = json.loads(location[0].text)\n",
    "    jsonnn_tree = objectpath.Tree(jt)\n",
    "    streetAddress_list.append(list(jsonnn_tree.execute('$..streetAddress'))[0])\n",
    "    addressLocality_list.append(list(jsonnn_tree.execute('$..addressLocality'))[0])\n",
    "    addressRegion_list.append(list(jsonnn_tree.execute('$..addressRegion'))[0])\n",
    "    postalCode_list.append(list(jsonnn_tree.execute('$..postalCode'))[0])\n",
    "    addressCountry_list.append(list(jsonnn_tree.execute('$..addressCountry'))[0])\n",
    "    full_address = page_html.findAll('div', {'class':\"institution-info__contact-detail institution-info__contact-detail--address\"})[0].text.strip()\n",
    "    full_address_list.append(full_address)\n",
    "    print ('{} out of {}'.format(len(full_address_list), len (web_address)), full_address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>name</th>\n",
       "      <th>number_students</th>\n",
       "      <th>student_staff_ratio</th>\n",
       "      <th>intl_students</th>\n",
       "      <th>female_male_ratio</th>\n",
       "      <th>overall_score</th>\n",
       "      <th>teaching_score</th>\n",
       "      <th>research_score</th>\n",
       "      <th>citations_score</th>\n",
       "      <th>industry_income_score</th>\n",
       "      <th>international_outlook_score</th>\n",
       "      <th>address</th>\n",
       "      <th>street_address</th>\n",
       "      <th>locality_address</th>\n",
       "      <th>region_address</th>\n",
       "      <th>postcode_address</th>\n",
       "      <th>country_address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [rank, name, number_students, student_staff_ratio, intl_students, female_male_ratio, overall_score, teaching_score, research_score, citations_score, industry_income_score, international_outlook_score, address, street_address, locality_address, region_address, postcode_address, country_address]\n",
       "Index: []"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'rank' : rank,\n",
    "    'name' : names,\n",
    "    'number_students' : number_students,\n",
    "    'student_staff_ratio' : student_staff_ratio,\n",
    "    'intl_students' : intl_students,\n",
    "    'female_male_ratio' : female_male_ratio,\n",
    "    'overall_score' : overall_score,\n",
    "    'teaching_score' : teaching_score,\n",
    "    'research_score' : research_score,\n",
    "    'citations_score' : citations_score,\n",
    "    'industry_income_score' : industry_income_score,\n",
    "    'international_outlook_score' : international_outlook_score,\n",
    "    'address' : full_address_list, \n",
    "    'street_address' : streetAddress_list,\n",
    "    'locality_address' : addressLocality_list,\n",
    "    'region_address' : addressRegion_list,\n",
    "    'postcode_address' : postalCode_list,\n",
    "    'country_address' : addressCountry_list\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# После создания DataFrame\n",
    "df = df.astype(str)  # Все колонки становятся строками\n",
    "\n",
    "# Потом уже делай replace\n",
    "df['intl_students'] = df['intl_students'].str.replace('%', '', regex=False)\n",
    "df['rank'] = df['rank'].str.replace(r'\\-\\d+|\\*', '', regex=True)\n",
    "df['overall_score'] = df['overall_score'].str.replace(r'.*\\=–', '', regex=True)\n",
    "df['number_students'] = df['number_students'].str.replace(',', '', regex=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('uni_02.csv', encoding='utf-16', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
